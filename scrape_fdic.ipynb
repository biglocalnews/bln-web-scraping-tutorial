{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63269a9b-235e-4ce5-a65d-a3fec93508b0",
   "metadata": {},
   "source": [
    "# Beginnning web scraping: The FDIC's list of failed banks\n",
    "\n",
    "This notebook walks you through the basics of web scraping by extracting the list of the Federal Deposit Insurance Commmision's [list of failed banks](https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/). For the purposes of the exercise, we are going to ignore the fact the data can be downloaded directly.\n",
    "\n",
    "## Import libraries\n",
    "\n",
    "You will need three libraries to scrape the website: [csv](https://docs.python.org/3/library/csv.html), [Requests](https://requests.readthedocs.io/en/latest/) and [BeautifulSoup4](https://beautiful-soup-4.readthedocs.io/en/latest/).\n",
    "- **csv**: This library handles the reading and writing of CSV files. It is part of the standard library, meaning it comes packaged with Python unlike the other two libraries.\n",
    "- **Requests**: Requests is what you will use to actually get the webpage from the Internet. It needs to be installed before you can use it - `pip install requests`.\n",
    "- **BeautifulSoup4**: Also known as **bs4**, this library is used to parse HTML and extract data from it. It also needs to be installed before use - `pip install bs4`.\n",
    "\n",
    "If you have this notebook running in Jupyter Lab, you should have all the needed libraries installed. If not, refer to the [README](./README.md) in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44e97a3a-8326-4c56-96b5-1447e17b998c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6994b7fb-4753-4567-a723-723ba9410346",
   "metadata": {},
   "source": [
    "## Making a web request\n",
    "\n",
    "The first step of each scrape is requesting the web page we want to extract information from. We do this providing a url and using Requests to make either a **get** or **post** request.\n",
    "> A **get** request is the most common type of request. It includes all the information a web server needs to return content to your browser in the url. Another common type of request is a **post** request. In this case additional information needs to be collected and sent to the web server. This is most often done through a form filled out by the user - a common example would be the search field on Google.\n",
    "\n",
    "To make the web request store the url in a variable called `url`. Next we make our web request using the `requests.get()` method and assign that to a variable called `response`. Finally, we check to make sure there were no problems making the request using the `response.raise_for_status()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7885a993-284b-4d1b-9218-e8ed9ec4361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/'\n",
    "\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69952641-a57f-4c24-9923-edad8b9743df",
   "metadata": {},
   "source": [
    "The `response` variable now contains a bunch of information about the web request we just made, but for now we are only interested in the HTML we just downloaded.\n",
    "\n",
    "## What is HTML\n",
    "\n",
    "[HTML](https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/HTML_basics) or HyperText Markup Language, is what is used to display content on a web page. It consists of a series of **elements** or **tags** to describe how information should be structured on a page. These tags can be recognized by the opening `<` and closing `>` angled brackets. Tags can be nested within each other, creating a heirarchy or tree of the content. A very basic web page looks like this:\n",
    "```html\n",
    "<html>\n",
    "  <head>\n",
    "    <title>This is an example page</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <p>This text is stored in a paragraph element.</p>\n",
    "  </body>\n",
    "</html>\n",
    "```\n",
    "In this example tags operate in pairs. The `<html>` tag signifies the beginning of the HTML while the `</html>` tag siginifies the end. Everything in between those two tags is considered part of the HTML and the web page. Everything between `<head>` and `</head>` contains metadata about the page while everything in between `<body>` and `</body>` is the content actually displayed on the page by a web browser. In this case it is a single paragraph, denoted by the `<p>` tag.\n",
    "\n",
    "Note the indentation, it indicates the heirarchy of content - the `<title>` tag belongs in the `<head>` of the page, but not the `<body>`. This is also called the tree.\n",
    "\n",
    "## Parsing HTML\n",
    "\n",
    "The HTML we just downloaded is stored in `response.text`. We need to load that into a parser so we can easily navigate it and extract the information we are looking for. To do this we load the data into BeautifulSoup and assigning it to a variable called `soup`. We are also going to create two new variables and assign empty lists to them. The `fieldnames` variable will hold a list of column names and `results` will hold all of our parsed data after we extract it from the web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12f2730b-42f1-4f39-9f76-798391b7de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text)\n",
    "\n",
    "fieldnames = []\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3d32c0-dc6b-4395-ad0a-4fd2f020d46d",
   "metadata": {},
   "source": [
    "### Extracting the table\n",
    "\n",
    "The information we want to extract is in a table on the web page. A table is often structured like this in HTML:\n",
    "\n",
    "```html\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr> \n",
    "            <th>Pet Owner</th>\n",
    "            <th>Pet Type</th>\n",
    "            <th>Pet Name</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Eric</td>\n",
    "            <td>Dog</td>\n",
    "            <td>Marco</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Joe</td>\n",
    "            <td>Dog</td>\n",
    "            <td>Leopold</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Cheryl</td>\n",
    "            <td>Dog</td>\n",
    "            <td>Tank</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "```\n",
    "The table's column names are stored in `<thead>` while the actual data is stored in `<tbody>`. Each row of the table is signified by a `<tr>` tag. Fieldnames are enclosed in `<th>` tags while each individual data point is stored in `<td>` tags.\n",
    "\n",
    "We can use this structure to start finding and extracting the data. Start by finding the table itself using the `soup.find()` method. There is only one `<table>` on this example so we can use `soup.find()` safely. If there were more than one table on the page, it would only return the first `<table>` element it finds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15865553-0c14-4222-ada6-f603f1968f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find('table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0863343-2ca8-46f4-a94c-9aeed931848d",
   "metadata": {},
   "source": [
    "### Extracting field names\n",
    "Next use the information stored in the `table` variable to find the `<thead>` element containing the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33e7e9bb-c075-4adc-94af-c11f1ebddec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "thead = table.find('thead')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0cedf7-813c-416d-8d25-972857190a66",
   "metadata": {},
   "source": [
    "Here is a view of what the HTML in the table head looks like:\n",
    "```html\n",
    "<thead class=\"dataTables-content-header bg-blue\">\n",
    "    <tr>\n",
    "        <th class=\"text-no-wrap text-left padding-left-2 padding-right-105 padding-top-2 padding-bottom-1\">\n",
    "            <p class=\"font-serif-xs text-light margin-0 padding-0 text-white\">\n",
    "                <span class=\"dtfullname\">Bank Name</span>\n",
    "                <span class=\"dtmobilename\">Bank</span>\n",
    "            </p>\n",
    "        </th>\n",
    "        <th class=\"text-no-wrap text-left padding-left-2 padding-right-105 padding-top-2 padding-bottom-1\">\n",
    "            <p class=\"font-serif-xs text-light margin-0 padding-0 text-white\">\n",
    "                <span class=\"dtfullname\">City</span>\n",
    "                <span class=\"dtmobilename\">City</span>\n",
    "            </p>\n",
    "        </th>\n",
    "        <th class=\"text-no-wrap text-left padding-left-2 desktop:padding-left-1 padding-right-105 padding-top-2 padding-bottom-1\">\n",
    "            <p class=\"font-serif-xs text-light margin-0 padding-0 text-white\">\n",
    "                <span class=\"dtfullname\">State</span>\n",
    "                <span class=\"dtmobilename\">St</span>\n",
    "            </p>\n",
    "        </th>\n",
    "        <th class=\"text-no-wrap text-left padding-left-2 padding-right-105 padding-top-2 padding-bottom-1\">\n",
    "            <p class=\"font-serif-xs text-light margin-0 padding-0 text-white\">\n",
    "                <span class=\"dtfullname\">Cert</span>\n",
    "                <span class=\"dtmobilename\">Cert</span>\n",
    "            </p>\n",
    "        </th>\n",
    "        <th class=\"text-no-wrap text-left padding-left-2 padding-right-105 padding-top-2 padding-bottom-1\">\n",
    "            <p class=\"font-serif-xs text-light margin-0 padding-0 text-white\">\n",
    "                <span class=\"dtfullname\">Acquiring Institution</span>\n",
    "                <span class=\"dtmobilename\">AI</span>\n",
    "            </p>\n",
    "        </th>\n",
    "        <th class=\"text-no-wrap text-left padding-left-2 padding-right-105 padding-top-2 padding-bottom-1\">\n",
    "            <p class=\"font-serif-xs text-light margin-0 padding-0 text-white\">\n",
    "                <span class=\"dtfullname\">Closing Date</span>\n",
    "                <span class=\"dtmobilename\">Closing</span>\n",
    "            </p>\n",
    "        </th>\n",
    "        <th class=\"text-border-right white text-no-wrap text-left padding-left-2 padding-right-105 padding-top-2 padding-bottom-1\">\n",
    "            <p class=\"font-serif-xs text-light margin-0 padding-0 text-white\">\n",
    "                <span class=\"dtfullname\">Fund</span>\n",
    "                <span class=\"dtmobilename\">Fund</span>\n",
    "            </p>\n",
    "        </th>\n",
    "    </tr>\n",
    "</thead>\n",
    "```\n",
    "\n",
    "Each column name is stored within the `<th>` tag, but there are two versions - one for desktop and another for mobile. We need to find all `<th>` tags and loop through them, extracting only the column name meant for desktop display since it contains the most information. We use `find_all()` instead of `find` since we want to capture every occurence of the `<th>` tag and not just the first one. For each `<th>` element we want to navigate the HTML heirarchy by selecting the `<p>` tag and the first `<span>` element - the one meant for desktop display. Then we extract the fieldname from the results and add it to our list of fieldnames. In BeautifulSoup we access the values we want using the `.text` attribute.\n",
    "\n",
    "We are going to add one last column name to `fieldnames` - **url**. This column will be for the link we see for each bank in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b3da52f-408c-404b-ba57-1bd40b713740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bank Name',\n",
       " 'City',\n",
       " 'State',\n",
       " 'Cert',\n",
       " 'Acquiring Institution',\n",
       " 'Closing Date',\n",
       " 'Fund',\n",
       " 'url']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for th in thead.find_all('th'):\n",
    "    fieldname = th.p.span.text\n",
    "    fieldnames.append(fieldname)\n",
    "\n",
    "fieldnames.append('url')\n",
    "fieldnames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfe8755-d720-4874-92ef-54c488bc1d39",
   "metadata": {},
   "source": [
    "This list of field names will go into our `results` list so they can be written out to our CSV later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "323c890e-ebe9-414d-8779-8ba01c3c176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append(fieldnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaf3f0f-7b34-4986-bf11-d42c3103d86b",
   "metadata": {},
   "source": [
    "### Exracting the data\n",
    "\n",
    "Next we are going to extract the data from the table. Start by isolating the `<tbody>` element and finding all rows (`<tr>` tags) within the table body. Notice again we are using `find` to find a single element and `find_all` to find multiple elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16c86035-4aa7-469c-8d67-438d070551f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbody = table.find('tbody')\n",
    "trs = tbody.find_all('tr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21a3006-e7b3-4b15-90b8-88ef956d9fd0",
   "metadata": {},
   "source": [
    "The `trs` variable is now a list of all the rows in the table's body. Here is what the first row looks like:\n",
    "\n",
    "```html\n",
    "<tr>\n",
    "    <td>\n",
    "        <a href=\"/resources/resolutions/bank-failures/failed-bank-list/citizensbank.html\">Citizens Bank</a>\n",
    "    </td>\n",
    "    <td>Sac City</td>\n",
    "    <td>IA</td>\n",
    "    <td>8758</td>\n",
    "    <td>Iowa Trust &amp; Savings Bank</td>\n",
    "    <td>November 3, 2023</td>\n",
    "    <td>10545</td>\n",
    "</tr>\n",
    "```\n",
    "\n",
    "For each row we need to do the following:\n",
    "- Create a variable `values` to hold the information we extract\n",
    "- Find all `<td>` tags which contains the actual data values\n",
    "- Loop through the `<td>` tags to extract each one's value and store it in the `values` variable\n",
    "- Find the link - `<a>` tag - in the row and extract its **href**, the address of the link.\n",
    "- Repair the link so it has the full url before adding it to our values.\n",
    "- Finally add the list of `values` to our `results` variable so it can be written out to a CSV.\n",
    "\n",
    "\n",
    "To do this we will need a couple of `for` loops, one within the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "364110ee-3b51-4a1c-84bf-d21fdbdb998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tr in trs:\n",
    "    values = []\n",
    "    tds = tr.find_all('td')\n",
    "\n",
    "    for td in tds:\n",
    "        values.append(td.text)\n",
    "\n",
    "    bank_link = tr.find('a')\n",
    "    href = bank_link['href']\n",
    "    bank_url = 'https://www.fdic.gov' + href\n",
    "    values.append(bank_url)\n",
    "\n",
    "    results.append(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946ff1f2-ee00-4143-afa5-965a37b01ba1",
   "metadata": {},
   "source": [
    "Do a couple of quick checks.\n",
    "\n",
    "Count number of records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "820c31cd-14a2-4453-9f72-5980729b0ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17726394-e5d0-4ff3-b433-baadecf23221",
   "metadata": {},
   "source": [
    "View the first five results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40d44645-01dd-4669-9282-75ea76466bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Bank Name',\n",
       "  'City',\n",
       "  'State',\n",
       "  'Cert',\n",
       "  'Acquiring Institution',\n",
       "  'Closing Date',\n",
       "  'Fund',\n",
       "  'url'],\n",
       " ['Citizens Bank',\n",
       "  'Sac City',\n",
       "  'IA',\n",
       "  '8758',\n",
       "  'Iowa Trust & Savings Bank',\n",
       "  'November 3, 2023',\n",
       "  '10545',\n",
       "  'https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/citizensbank.html'],\n",
       " ['Heartland Tri-State Bank',\n",
       "  'Elkhart',\n",
       "  'KS',\n",
       "  '25851',\n",
       "  'Dream First Bank, N.A.',\n",
       "  'July 28, 2023',\n",
       "  '10544',\n",
       "  'https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/heartlandtristate.html'],\n",
       " ['First Republic Bank',\n",
       "  'San Francisco',\n",
       "  'CA',\n",
       "  '59017',\n",
       "  'JPMorgan Chase Bank, N.A.',\n",
       "  'May 1, 2023',\n",
       "  '10543',\n",
       "  'https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/first-republic.html'],\n",
       " ['Signature Bank',\n",
       "  'New York',\n",
       "  'NY',\n",
       "  '57053',\n",
       "  'Flagstar Bank, N.A.',\n",
       "  'March 12, 2023',\n",
       "  '10540',\n",
       "  'https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/signature-ny.html']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f03a8d9-b8e1-462f-afb8-415c01f49e48",
   "metadata": {},
   "source": [
    "## Write out the results\n",
    "\n",
    "Now we are ready to write our data out to a CSV file. To do this we open up a file in write mode and use Python's **csv** library to write out the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f7c31ab-8de8-4ed1-a9dd-cb4f091d1276",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/raw/fdic_failed_banks.csv', 'w') as outfile:\n",
    "    output = csv.writer(outfile)\n",
    "    output.writerows(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5251baf-14f2-4f25-aecd-238956cdf18f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Congratulations!! You've written your first web scraper. Web sites are not always so easy to scrape, please reach out to Big Local News if you are having difficulty scraping a particular site and we can help answer any questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd58414-c476-40f6-b34a-764de25495ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
